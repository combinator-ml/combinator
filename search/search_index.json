{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Easy MLOps Stacks Sophisticated teams develop their MLOps stack from a combination of best of breed components. But building or spinning up stacks is incredibly difficult. This open source community exists to make combining them less of a headache. Deploy common MLOps stacks with a single command. Credit: Clemens Mewald How It Works Combinator.ml makes it easy to test drive, combine & deploy the stack that's best for you. Components, Stacks and Infrastructure Combinator components are infinitely composible ML products. We present a curated combination of components as stacks, although it is easy to develop your own bespoke stack. Infrastructure abstractions make it easy to deploy components and stacks. Find out more. Terraform is the common language we use to combine stacks. Don't worry if you're not familiar with it, we make it easy to get started with it. You can test drive (with the help of TestFaster ), spin up a local dev environment, or deploy to a cloud of your choice. Every component is published as a terraform module in an open source GitHub repo. Each stack is a terraform module too, which references the terraform modules of the components. We need your help to maintain these! Please contribute to the existing components or stacks, or create your own. Find out more. MLOps Categories Components typically fall into one ore more of the following MLOps catagories. Find out more. Category Description Data Store, manage, and move data. Explore Develop Develop and train models Explore Deploy Deploy models to extract value. Explore Monitor Understand the behavior of your models Explore Govern Manage, control, and audit your models. Explore Featured Stacks Stacks are opinionated combinations of components. Find out more. Here are a few examples: Kubeflow + MLflow By combinator.ml Kubeflow provides orchestration for notebooks, pipelines, and serving. This stack adds MLflow for model management and makes it easy to log models to MLflow from kubeflow notebooks and pipelines. View Minio + Pachyderm By combinator.ml Minio is an S3-compatible cloud-native data store. A Pachyderm cluster provides data lineage and pipelines. This stack makes it easy to spin up a cloud-native versioned controlled data store and pipelining tool. View Getting Started It's really easy to get started. Install Terraform Browse to a stack or component and follow the instructions to deploy. Get involved! Find us in #mlops-stacks on the MLOps.community Slack: Join Slack Or read more about contributing . Community Founders Luke Marsden - MLOps Consulting Kai Davenport - MLOps Consulting Phil Winder - Winder.ai - MLOps Consulting Demetrios Brinkmann - MLOps Community Dan Baker - AutoDB","title":"Easy MLOps Stacks"},{"location":"#easy-mlops-stacks","text":"Sophisticated teams develop their MLOps stack from a combination of best of breed components. But building or spinning up stacks is incredibly difficult. This open source community exists to make combining them less of a headache. Deploy common MLOps stacks with a single command. Credit: Clemens Mewald","title":"Easy MLOps Stacks"},{"location":"#how-it-works","text":"Combinator.ml makes it easy to test drive, combine & deploy the stack that's best for you.","title":"How It Works"},{"location":"#components-stacks-and-infrastructure","text":"Combinator components are infinitely composible ML products. We present a curated combination of components as stacks, although it is easy to develop your own bespoke stack. Infrastructure abstractions make it easy to deploy components and stacks. Find out more.","title":"Components, Stacks and Infrastructure"},{"location":"#_1","text":"Terraform is the common language we use to combine stacks. Don't worry if you're not familiar with it, we make it easy to get started with it. You can test drive (with the help of TestFaster ), spin up a local dev environment, or deploy to a cloud of your choice.","title":""},{"location":"#_2","text":"Every component is published as a terraform module in an open source GitHub repo. Each stack is a terraform module too, which references the terraform modules of the components. We need your help to maintain these! Please contribute to the existing components or stacks, or create your own. Find out more.","title":""},{"location":"#mlops-categories","text":"Components typically fall into one ore more of the following MLOps catagories. Find out more. Category Description Data Store, manage, and move data. Explore Develop Develop and train models Explore Deploy Deploy models to extract value. Explore Monitor Understand the behavior of your models Explore Govern Manage, control, and audit your models. Explore","title":"MLOps Categories"},{"location":"#featured-stacks","text":"Stacks are opinionated combinations of components. Find out more. Here are a few examples: Kubeflow + MLflow By combinator.ml Kubeflow provides orchestration for notebooks, pipelines, and serving. This stack adds MLflow for model management and makes it easy to log models to MLflow from kubeflow notebooks and pipelines. View Minio + Pachyderm By combinator.ml Minio is an S3-compatible cloud-native data store. A Pachyderm cluster provides data lineage and pipelines. This stack makes it easy to spin up a cloud-native versioned controlled data store and pipelining tool. View","title":"Featured Stacks"},{"location":"#getting-started","text":"It's really easy to get started. Install Terraform Browse to a stack or component and follow the instructions to deploy.","title":"Getting Started"},{"location":"#get-involved","text":"Find us in #mlops-stacks on the MLOps.community Slack: Join Slack Or read more about contributing .","title":"Get involved!"},{"location":"#community-founders","text":"Luke Marsden - MLOps Consulting Kai Davenport - MLOps Consulting Phil Winder - Winder.ai - MLOps Consulting Demetrios Brinkmann - MLOps Community Dan Baker - AutoDB","title":"Community Founders"},{"location":"CODE_OF_CONDUCT/","text":"Code of Conduct Our Pledge We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation. We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community. Our Standards Examples of behavior that contributes to a positive environment for our community include: Demonstrating empathy and kindness toward other people Being respectful of differing opinions, viewpoints, and experiences Giving and gracefully accepting constructive feedback Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience Focusing on what is best not just for us as individuals, but for the overall community Examples of unacceptable behavior include: The use of sexualized language or imagery, and sexual attention or advances of any kind Trolling, insulting or derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or email address, without their explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting Enforcement Responsibilities Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful. Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate. Scope This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Enforcement Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at phil@winderresearch.com . All complaints will be reviewed and investigated promptly and fairly. All community leaders are obligated to respect the privacy and security of the reporter of any incident. Enforcement Guidelines Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct: 1. Correction Community Impact : Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community. Consequence : A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested. 2. Warning Community Impact : A violation through a single incident or series of actions. Consequence : A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban. 3. Temporary Ban Community Impact : A serious violation of community standards, including sustained inappropriate behavior. Consequence : A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban. 4. Permanent Ban Community Impact : Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals. Consequence : A permanent ban from any sort of public interaction within the community. Attribution This Code of Conduct is adapted from the Contributor Covenant , version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html . Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder . For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq . Translations are available at https://www.contributor-covenant.org/translations .","title":"Code of Conduct"},{"location":"CODE_OF_CONDUCT/#code-of-conduct","text":"","title":"Code of Conduct"},{"location":"CODE_OF_CONDUCT/#our-pledge","text":"We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation. We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.","title":"Our Pledge"},{"location":"CODE_OF_CONDUCT/#our-standards","text":"Examples of behavior that contributes to a positive environment for our community include: Demonstrating empathy and kindness toward other people Being respectful of differing opinions, viewpoints, and experiences Giving and gracefully accepting constructive feedback Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience Focusing on what is best not just for us as individuals, but for the overall community Examples of unacceptable behavior include: The use of sexualized language or imagery, and sexual attention or advances of any kind Trolling, insulting or derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or email address, without their explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting","title":"Our Standards"},{"location":"CODE_OF_CONDUCT/#enforcement-responsibilities","text":"Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful. Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.","title":"Enforcement Responsibilities"},{"location":"CODE_OF_CONDUCT/#scope","text":"This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.","title":"Scope"},{"location":"CODE_OF_CONDUCT/#enforcement","text":"Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at phil@winderresearch.com . All complaints will be reviewed and investigated promptly and fairly. All community leaders are obligated to respect the privacy and security of the reporter of any incident.","title":"Enforcement"},{"location":"CODE_OF_CONDUCT/#enforcement-guidelines","text":"Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:","title":"Enforcement Guidelines"},{"location":"CODE_OF_CONDUCT/#1-correction","text":"Community Impact : Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community. Consequence : A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.","title":"1. Correction"},{"location":"CODE_OF_CONDUCT/#2-warning","text":"Community Impact : A violation through a single incident or series of actions. Consequence : A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.","title":"2. Warning"},{"location":"CODE_OF_CONDUCT/#3-temporary-ban","text":"Community Impact : A serious violation of community standards, including sustained inappropriate behavior. Consequence : A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.","title":"3. Temporary Ban"},{"location":"CODE_OF_CONDUCT/#4-permanent-ban","text":"Community Impact : Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals. Consequence : A permanent ban from any sort of public interaction within the community.","title":"4. Permanent Ban"},{"location":"CODE_OF_CONDUCT/#attribution","text":"This Code of Conduct is adapted from the Contributor Covenant , version 2.0, available at https://www.contributor-covenant.org/version/2/0/code_of_conduct.html . Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder . For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq . Translations are available at https://www.contributor-covenant.org/translations .","title":"Attribution"},{"location":"CONTRIBUTING/","text":"Contributing First off, thanks for taking the time to contribute. You're amazing! \ud83c\udf89 \ud83d\ude18 \u2728 If at any point you need any help, the best way to get in touch with someone is in #mlops-stacks channel on the MLOps.community Slack. Join Slack The following is a set of guidelines for contributing. These are mostly guidelines, not rules. Use your best judgment, and feel free to propose changes to this document in a pull request. Contributing Components and Stacks Reporting Bugs Suggesting Feature Requests or Enhancements Pull Requests Check Code Style Test Commit Yes! Pull request Code of Conduct Contributing Components and Stacks See the tutorial . Reporting Bugs Bugs are tracked as GitHub issues, tagged with a bug label. Search before you create an issue. When you create an issue, please provide the following information by filling in the template. Core documentation or website issues should be reported to the combinator repository Component or stack issues should be reported to the respective repository, because these may not be owned by the core contributors Suggesting Feature Requests or Enhancements Enhancements or feature requests are tracked as GitHub issues, tagged with an enhancement label. When you create an issue, please provide the following information by filling in the template. Enhancements to combinator in general (e.g. new stacks, new components, new ideas) should be reported to the combinator repository Enhancements to components or stacks should be reported to the respective repository, because these may not be owned by the core contributors Pull Requests Check Code Style Run make lint and make sure all the tests pass. Test Run make test and verify all the tests pass. Commit Commit Message Format <Type>: Short description (fix #1234) Longer description here if necessary BREAKING CHANGE: only contain breaking change Type Must be one of the following: feat : A new feature fix : A bug fix breaking : A breaking change docs : Documentation only changes style : Changes that do not affect the meaning of the code (white-space, formatting, missing semi-colons, etc) refactor : A code change that neither fixes a bug nor adds a feature perf : A code change that improves performance test : Adding missing or correcting existing tests chore : Changes to the build process or auxiliary tools and libraries such as documentation generation revert : Reverting changes Subject use the imperative, present tense: \"change\" not \"changed\" nor \"changes\" don't capitalize the first letter no dot (.) at the end reference GitHub issues at the end. If the commit doesn\u2019t completely fix the issue, then use (refs #1234) instead of (fixes #1234) . Body use the imperative, present tense: \"change\" not \"changed\" nor \"changes\". the motivation for the change and contrast this with previous behavior. Yes! Pull request Make your pull request, then describe your changes. Title Follow other PR title format on below. <Type>: Short Description (fix #111) <Type>: Short Description (fix #123, #111, #122) <Type>: Short Description (ref #111) * use present tense: 'change' not 'changed' or 'changes' Description If it has related to issues, add links to the issues (like #123 ) in the description. Code of Conduct See the CODE_OF_CONDUCT.md .","title":"Contributing"},{"location":"CONTRIBUTING/#contributing","text":"First off, thanks for taking the time to contribute. You're amazing! \ud83c\udf89 \ud83d\ude18 \u2728 If at any point you need any help, the best way to get in touch with someone is in #mlops-stacks channel on the MLOps.community Slack. Join Slack The following is a set of guidelines for contributing. These are mostly guidelines, not rules. Use your best judgment, and feel free to propose changes to this document in a pull request. Contributing Components and Stacks Reporting Bugs Suggesting Feature Requests or Enhancements Pull Requests Check Code Style Test Commit Yes! Pull request Code of Conduct","title":"Contributing"},{"location":"CONTRIBUTING/#contributing-components-and-stacks","text":"See the tutorial .","title":"Contributing Components and Stacks"},{"location":"CONTRIBUTING/#reporting-bugs","text":"Bugs are tracked as GitHub issues, tagged with a bug label. Search before you create an issue. When you create an issue, please provide the following information by filling in the template. Core documentation or website issues should be reported to the combinator repository Component or stack issues should be reported to the respective repository, because these may not be owned by the core contributors","title":"Reporting Bugs"},{"location":"CONTRIBUTING/#suggesting-feature-requests-or-enhancements","text":"Enhancements or feature requests are tracked as GitHub issues, tagged with an enhancement label. When you create an issue, please provide the following information by filling in the template. Enhancements to combinator in general (e.g. new stacks, new components, new ideas) should be reported to the combinator repository Enhancements to components or stacks should be reported to the respective repository, because these may not be owned by the core contributors","title":"Suggesting Feature Requests or Enhancements"},{"location":"CONTRIBUTING/#pull-requests","text":"","title":"Pull Requests"},{"location":"CONTRIBUTING/#check-code-style","text":"Run make lint and make sure all the tests pass.","title":"Check Code Style"},{"location":"CONTRIBUTING/#test","text":"Run make test and verify all the tests pass.","title":"Test"},{"location":"CONTRIBUTING/#commit","text":"","title":"Commit"},{"location":"CONTRIBUTING/#commit-message-format","text":"<Type>: Short description (fix #1234) Longer description here if necessary BREAKING CHANGE: only contain breaking change","title":"Commit Message Format"},{"location":"CONTRIBUTING/#type","text":"Must be one of the following: feat : A new feature fix : A bug fix breaking : A breaking change docs : Documentation only changes style : Changes that do not affect the meaning of the code (white-space, formatting, missing semi-colons, etc) refactor : A code change that neither fixes a bug nor adds a feature perf : A code change that improves performance test : Adding missing or correcting existing tests chore : Changes to the build process or auxiliary tools and libraries such as documentation generation revert : Reverting changes","title":"Type"},{"location":"CONTRIBUTING/#subject","text":"use the imperative, present tense: \"change\" not \"changed\" nor \"changes\" don't capitalize the first letter no dot (.) at the end reference GitHub issues at the end. If the commit doesn\u2019t completely fix the issue, then use (refs #1234) instead of (fixes #1234) .","title":"Subject"},{"location":"CONTRIBUTING/#body","text":"use the imperative, present tense: \"change\" not \"changed\" nor \"changes\". the motivation for the change and contrast this with previous behavior.","title":"Body"},{"location":"CONTRIBUTING/#yes-pull-request","text":"Make your pull request, then describe your changes.","title":"Yes! Pull request"},{"location":"CONTRIBUTING/#title","text":"Follow other PR title format on below. <Type>: Short Description (fix #111) <Type>: Short Description (fix #123, #111, #122) <Type>: Short Description (ref #111) * use present tense: 'change' not 'changed' or 'changes'","title":"Title"},{"location":"CONTRIBUTING/#description","text":"If it has related to issues, add links to the issues (like #123 ) in the description.","title":"Description"},{"location":"CONTRIBUTING/#code-of-conduct","text":"See the CODE_OF_CONDUCT.md .","title":"Code of Conduct"},{"location":"design/","text":"Design and Standards Imagine this. You've been given the task of testing a new piece of ML software. Maybe you want to integrate it into your stack or maybe you just want to try it out. In our experience we spent a non-trivial amount of time attempting to get the stack running, especially when there are multiple components involved. Combinator attempts to simplify the deployment of common ML stacks. We hope that over time these could become \"standardized\" stacks, which people use as the ground truth when, for example, stating dependencies. To achieve this goal we need to define some principals, otherwise there can be no standardization. This isn't formal, like in the bureaucratic sense of the word, but it does aim to be a pattern, to ensure consistency throughout the codebase and make it easier for users to comprehend. To than end, this document serves as a set of opinions (which may change over time) that all code must adhere to. Overview There are three types of code in this project: components, stacks, and infra. Components provide the smallest possible amount of functionality. Ideally they solve one specific MLOps-related problem and are usually a single project - obvious exceptions are things like Kubeflow. Components are flexible enough to be reused in several stacks. Stacks are a combination of components that produce a suite of functionality. Stacks are reusable modules too, but they should only comprise of low-level components and any requisite glue. Infrastructure represents infrastructure components that are required to run stacks and components. Typically this defaults to Kubernetes, but all cloud resources are allowed. All three are managed as terraform code, to make it portable enough to work with any cloud, and to be able to install and/or control all elements throughout the stack. This allows combinator to use best-of-breed resources, like provision Kubernetes clusters with GPUs, or applications through the use of Helm charts. And all with a single dependency, Terraform. Standards All components, stacks, and infrastructure must: Be packaged as a terraform module. Be published in the Terraform Registry to make it easier to install. Target Kubernetes, where possible. Have an accompanying page on https://github.com/combinator-ml/combinator All components, stacks, and infrastructure should: Be concise. For example, if an application can run in memory, it should; include extra storage in a stack. But if it needs a database to work, you must bundle the database into the component if it can\u2019t deploy one itself. Only expose options for variations if they have the same interface. When a component or stack can have multiple sub-components, only include those that provide a like-for-like experience. For example, including the option to select Minio or S3 is great. But selecting whether to include Kubeflow or Sagemaker is not; that should be a separate stack. Expose all pertinent parameters, in anticipation of future use cases. Use semantic versioning to inform users about changes. Be packaged with working examples, to allow users to spin up the component quickly. Components must not: Make breaking changes (unless impossible to avoid). Where breaking changes are necessary, bump the major version and make a big scary announcement. Creating Components or Stacks There is little practical difference between components and stacks, other than the level of abstraction. Therefore we recommend the following layout (a basic terraform module layout): \u251c\u2500\u2500 .github # (Optional) Use github actions to ensure quality \u2502 \u2514\u2500\u2500 workflows \u2502 \u251c\u2500\u2500 documentation.yaml \u2502 \u2514\u2500\u2500 terraform.yaml \u251c\u2500\u2500 .gitignore \u251c\u2500\u2500 .header.md # (Optional) Part of the final README.md \u251c\u2500\u2500 .terraform-docs.yml # (Optional) A way of automatically generating terraform docs from a github action \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 README.md \u251c\u2500\u2500 examples \u2502 \u2514\u2500\u2500 basic \u2502 \u2514\u2500\u2500 main.tf \u251c\u2500\u2500 locals.tf \u251c\u2500\u2500 component-technology-0.tf \u251c\u2500\u2500 component-technology-1.tf \u251c\u2500\u2500 outputs.tf \u251c\u2500\u2500 providers.tf \u2514\u2500\u2500 variables.tf Tutorial We've created some helpful templates to make it easier to get started developing a component or stack. Follow the steps to get started. 1. Create a Repo from a Template First create your new repository and base it upon the combinator template . If you are working within the combinator organization, then you can use Github's template functionality. Otherwise, just merge the repo into yours. The template provides the following functionality: Good initial structure Github actions to: publish docs to the main combinator site, generate terraform documentation, create release notes, lint, test the terraform code 2. Release to the Terraform Registry Create your first release, 0.0.0 by convention, browse to the Terraform registry and publish a new module . Once generated it should show your template repo. 3. Update the Code and Examples Now you need to go through all the Terraform code and implement your component. Take a look at the other components for examples using Helm to install your application, or leverage other modules to create a stack, for example. 4. Update the Documentation Now go through the .header.md file and update your README. This file will be pushed to the combinator.ml website, so make sure it makes sense outside of the context of the repository. Browse to the combinator-docs.yml github action and update the COMBINATOR_DOCS_FILE environmental variable to point to the location where you want your documentation to appear in the website heirarchy. For example, the Feast component is set to: docs/components/data/feast.md . The directory must already exist. If you are working within the combinator-ml organization then the combinator docs github action will start to work once merged. Users Outside of the Combinator-ml Organization But if you are outside of the combinator-ml organization then you will need to create a Github Secret called CI_TOKEN with a personal Github access token that has the single permission: public_repo . This delegates permission to create a PR using your access credentials. 5. Create a PR Push your code to a branch and create a PR. This will kick off the various Github actions. This step is important because if you just push to master, it won't regenerate the documentation. Make sure all the actions pass, then merge the PR in your repository. You should now have a valid README.md. 6. Merge the Combinator Docs Now browse to the combinator repository and look at the PR that your documentation action created. Make sure everything is in order and request a merge from one of the project's admins. Don't forget to add your page to the mkdocs settings file if the component is new, because only the pages listed in this file are added to the website. You might also want to update any introductory pages or cross-references. From then on, whenever you update your README.md, it will automatically raise a PR in the combinator repository . 7. Celebrate and Share Now share your success! Tell us about the experience on Slack! Share with your colleagues! Share on social media! Spread the love!","title":"Design and Standards"},{"location":"design/#design-and-standards","text":"Imagine this. You've been given the task of testing a new piece of ML software. Maybe you want to integrate it into your stack or maybe you just want to try it out. In our experience we spent a non-trivial amount of time attempting to get the stack running, especially when there are multiple components involved. Combinator attempts to simplify the deployment of common ML stacks. We hope that over time these could become \"standardized\" stacks, which people use as the ground truth when, for example, stating dependencies. To achieve this goal we need to define some principals, otherwise there can be no standardization. This isn't formal, like in the bureaucratic sense of the word, but it does aim to be a pattern, to ensure consistency throughout the codebase and make it easier for users to comprehend. To than end, this document serves as a set of opinions (which may change over time) that all code must adhere to.","title":"Design and Standards"},{"location":"design/#overview","text":"There are three types of code in this project: components, stacks, and infra. Components provide the smallest possible amount of functionality. Ideally they solve one specific MLOps-related problem and are usually a single project - obvious exceptions are things like Kubeflow. Components are flexible enough to be reused in several stacks. Stacks are a combination of components that produce a suite of functionality. Stacks are reusable modules too, but they should only comprise of low-level components and any requisite glue. Infrastructure represents infrastructure components that are required to run stacks and components. Typically this defaults to Kubernetes, but all cloud resources are allowed. All three are managed as terraform code, to make it portable enough to work with any cloud, and to be able to install and/or control all elements throughout the stack. This allows combinator to use best-of-breed resources, like provision Kubernetes clusters with GPUs, or applications through the use of Helm charts. And all with a single dependency, Terraform.","title":"Overview"},{"location":"design/#standards","text":"All components, stacks, and infrastructure must: Be packaged as a terraform module. Be published in the Terraform Registry to make it easier to install. Target Kubernetes, where possible. Have an accompanying page on https://github.com/combinator-ml/combinator All components, stacks, and infrastructure should: Be concise. For example, if an application can run in memory, it should; include extra storage in a stack. But if it needs a database to work, you must bundle the database into the component if it can\u2019t deploy one itself. Only expose options for variations if they have the same interface. When a component or stack can have multiple sub-components, only include those that provide a like-for-like experience. For example, including the option to select Minio or S3 is great. But selecting whether to include Kubeflow or Sagemaker is not; that should be a separate stack. Expose all pertinent parameters, in anticipation of future use cases. Use semantic versioning to inform users about changes. Be packaged with working examples, to allow users to spin up the component quickly. Components must not: Make breaking changes (unless impossible to avoid). Where breaking changes are necessary, bump the major version and make a big scary announcement.","title":"Standards"},{"location":"design/#creating-components-or-stacks","text":"There is little practical difference between components and stacks, other than the level of abstraction. Therefore we recommend the following layout (a basic terraform module layout): \u251c\u2500\u2500 .github # (Optional) Use github actions to ensure quality \u2502 \u2514\u2500\u2500 workflows \u2502 \u251c\u2500\u2500 documentation.yaml \u2502 \u2514\u2500\u2500 terraform.yaml \u251c\u2500\u2500 .gitignore \u251c\u2500\u2500 .header.md # (Optional) Part of the final README.md \u251c\u2500\u2500 .terraform-docs.yml # (Optional) A way of automatically generating terraform docs from a github action \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 README.md \u251c\u2500\u2500 examples \u2502 \u2514\u2500\u2500 basic \u2502 \u2514\u2500\u2500 main.tf \u251c\u2500\u2500 locals.tf \u251c\u2500\u2500 component-technology-0.tf \u251c\u2500\u2500 component-technology-1.tf \u251c\u2500\u2500 outputs.tf \u251c\u2500\u2500 providers.tf \u2514\u2500\u2500 variables.tf","title":"Creating Components or Stacks"},{"location":"design/#tutorial","text":"We've created some helpful templates to make it easier to get started developing a component or stack. Follow the steps to get started.","title":"Tutorial"},{"location":"design/#1-create-a-repo-from-a-template","text":"First create your new repository and base it upon the combinator template . If you are working within the combinator organization, then you can use Github's template functionality. Otherwise, just merge the repo into yours. The template provides the following functionality: Good initial structure Github actions to: publish docs to the main combinator site, generate terraform documentation, create release notes, lint, test the terraform code","title":"1. Create a Repo from a Template"},{"location":"design/#2-release-to-the-terraform-registry","text":"Create your first release, 0.0.0 by convention, browse to the Terraform registry and publish a new module . Once generated it should show your template repo.","title":"2. Release to the Terraform Registry"},{"location":"design/#3-update-the-code-and-examples","text":"Now you need to go through all the Terraform code and implement your component. Take a look at the other components for examples using Helm to install your application, or leverage other modules to create a stack, for example.","title":"3. Update the Code and Examples"},{"location":"design/#4-update-the-documentation","text":"Now go through the .header.md file and update your README. This file will be pushed to the combinator.ml website, so make sure it makes sense outside of the context of the repository. Browse to the combinator-docs.yml github action and update the COMBINATOR_DOCS_FILE environmental variable to point to the location where you want your documentation to appear in the website heirarchy. For example, the Feast component is set to: docs/components/data/feast.md . The directory must already exist. If you are working within the combinator-ml organization then the combinator docs github action will start to work once merged.","title":"4. Update the Documentation"},{"location":"design/#users-outside-of-the-combinator-ml-organization","text":"But if you are outside of the combinator-ml organization then you will need to create a Github Secret called CI_TOKEN with a personal Github access token that has the single permission: public_repo . This delegates permission to create a PR using your access credentials.","title":"Users Outside of the Combinator-ml Organization"},{"location":"design/#5-create-a-pr","text":"Push your code to a branch and create a PR. This will kick off the various Github actions. This step is important because if you just push to master, it won't regenerate the documentation. Make sure all the actions pass, then merge the PR in your repository. You should now have a valid README.md.","title":"5. Create a PR"},{"location":"design/#6-merge-the-combinator-docs","text":"Now browse to the combinator repository and look at the PR that your documentation action created. Make sure everything is in order and request a merge from one of the project's admins. Don't forget to add your page to the mkdocs settings file if the component is new, because only the pages listed in this file are added to the website. You might also want to update any introductory pages or cross-references. From then on, whenever you update your README.md, it will automatically raise a PR in the combinator repository .","title":"6. Merge the Combinator Docs"},{"location":"design/#7-celebrate-and-share","text":"Now share your success! Tell us about the experience on Slack! Share with your colleagues! Share on social media! Spread the love!","title":"7. Celebrate and Share"},{"location":"terrachain/","text":"Terrachain (alpha) The purpose of the \"terrachain\" format is to enable the execution of a sequence of terraform modules, feeding the output of one into the input for another. In particular this enables workflows where one module creates a Kubernetes cluster, and a later module deploys into that Kubernetes cluster. See the Terraform documentation and this GitHub issue for motivation. Example of a configuration that terrachain can apply: apiVersion: combinator.ml/v1alpha1 modules: # The modules get terraform-applied in order, with the outputs from each # being passed into the inputs to the next. It is up to the module authors to # ensure the inputs & outputs match up in such a way that they are mutually # compatible. - name: aks repo: https://github.com/combinator-ml/terraform-azure-kubernetes args: autoscaling: on - name: kfp repo: https://github.com/combinator-ml/terraform-k8s-kfp Or a simpler example, just installing mlflow onto an existing Kubernetes cluster (assumes kubeconfig file in default location or KUBECONFIG or similar env var is set): apiVersion: combinator.ml/v1alpha1 modules: - name: mlflow repo: https://github.com/combinator-ml/terraform-k8s-mlflow The terrachain format can be consumed by the Combinator app and the SAME project with: same init -f As a special case, if a provider outputs a variable called kubeconfig_contents , that file will be written to a temporary file and then the path to the temporary file passed to later modules in both KUBECONFIG and KUBE_CONFIG_PATH environment variables for convenience. If no KUBE_CONFIG_PATH variable is set, and no previous module outputs a kubeconfig_contents , terrachain will set it to ~/.kube/config to workaround this issue . There are some sample terrachain configs at https://github.com/combinator-ml/stacks .","title":"Terrachain (alpha)"},{"location":"terrachain/#terrachain-alpha","text":"The purpose of the \"terrachain\" format is to enable the execution of a sequence of terraform modules, feeding the output of one into the input for another. In particular this enables workflows where one module creates a Kubernetes cluster, and a later module deploys into that Kubernetes cluster. See the Terraform documentation and this GitHub issue for motivation. Example of a configuration that terrachain can apply: apiVersion: combinator.ml/v1alpha1 modules: # The modules get terraform-applied in order, with the outputs from each # being passed into the inputs to the next. It is up to the module authors to # ensure the inputs & outputs match up in such a way that they are mutually # compatible. - name: aks repo: https://github.com/combinator-ml/terraform-azure-kubernetes args: autoscaling: on - name: kfp repo: https://github.com/combinator-ml/terraform-k8s-kfp Or a simpler example, just installing mlflow onto an existing Kubernetes cluster (assumes kubeconfig file in default location or KUBECONFIG or similar env var is set): apiVersion: combinator.ml/v1alpha1 modules: - name: mlflow repo: https://github.com/combinator-ml/terraform-k8s-mlflow The terrachain format can be consumed by the Combinator app and the SAME project with: same init -f As a special case, if a provider outputs a variable called kubeconfig_contents , that file will be written to a temporary file and then the path to the temporary file passed to later modules in both KUBECONFIG and KUBE_CONFIG_PATH environment variables for convenience. If no KUBE_CONFIG_PATH variable is set, and no previous module outputs a kubeconfig_contents , terrachain will set it to ~/.kube/config to workaround this issue . There are some sample terrachain configs at https://github.com/combinator-ml/stacks .","title":"Terrachain (alpha)"},{"location":"components/introduction/","text":"Introduction Combinator components are intend to be best-of-breed point-solutions for specific ML problems. A combination of components build a stack. Since there are multiple components that attempt to solve the same problem, components can be organized into catagories. These are: Data Develop Deploy Monitor Govern Undoubtedly there are components that cross boundaries. When this happens, they are listed in multiple catagories. Data Minio - S3 API compatible cloud-native storage Pachyderm - Data lineage (version control) Feast - Feature store Istio - Service mesh Develop Pachyderm - Pipelines Jupyter - Notebooks Kubeflow - Pipelines, Notebooks, Training Deploy KFServing - Serving Seldon - Serving and monitoring Monitor Boxkite - Cloud-native model monitoring Seldon - Serving and monitoring Govern MLFlow - Model management","title":"Introduction"},{"location":"components/introduction/#introduction","text":"Combinator components are intend to be best-of-breed point-solutions for specific ML problems. A combination of components build a stack. Since there are multiple components that attempt to solve the same problem, components can be organized into catagories. These are: Data Develop Deploy Monitor Govern Undoubtedly there are components that cross boundaries. When this happens, they are listed in multiple catagories.","title":"Introduction"},{"location":"components/introduction/#data","text":"Minio - S3 API compatible cloud-native storage Pachyderm - Data lineage (version control) Feast - Feature store Istio - Service mesh","title":"Data"},{"location":"components/introduction/#develop","text":"Pachyderm - Pipelines Jupyter - Notebooks Kubeflow - Pipelines, Notebooks, Training","title":"Develop"},{"location":"components/introduction/#deploy","text":"KFServing - Serving Seldon - Serving and monitoring","title":"Deploy"},{"location":"components/introduction/#monitor","text":"Boxkite - Cloud-native model monitoring Seldon - Serving and monitoring","title":"Monitor"},{"location":"components/introduction/#govern","text":"MLFlow - Model management","title":"Govern"},{"location":"components/data/feast/","text":"Feast tl; dr; A combinator data component that installs Feast , a feature store. Introduction Test Drive Usage Introduction Feast is an open-source feature store. A feature store allows you to manage, govern, and trace features derived from raw data. This is useful because it helps to unify and standardise, which reduces waste, improves quality, and makes models more reproducible. Feast does not perform any computation. You can think of it as a meta-database; a database that manages other databases. It effectively creates a cache of feature data, keyed by time. The Feast libraries and CLIs provide a consistent way of pushing or streaming new data into the cache. Downstream systems use a similar interface to access point-in-time data. Learn more about feast in the documentation. Test Drive The fastest way to get started is to use the test drive functionality provided by TestFaster . Click on the \"Launch Test Drive\" button below (opens a new window). Launch Test Drive Launch Jupyter Once the component has launched, click on the Jupyter link. Feast does not come with a UI. You will use Jupyter to interact with Feast via its API. Example Notebook Once inside Jupyter, browse to the minimal notebook, which is the official example . Follow the instructions in the notebook. Usage Prerequisites Start by preparing your Kubernetes cluster using one of the infrastructure components or use your own cluster. Component Usage module \"feast\" { source = \"combinator-ml/feast/k8s\" # Optional settings go here } See the full configuration options below. Requirements No requirements. Providers Name Version helm n/a kubernetes n/a random n/a Modules No Modules. Resources Name helm_release kubernetes_namespace kubernetes_secret random_password Inputs Name Description Type Default Required name_prefix Prefix to be used when naming the different components of Feast string \"combinator\" no namespace (Optional) The namespace to install into. Defaults to feast. string \"feast\" no Outputs No output.","title":"Feast"},{"location":"components/data/feast/#feast","text":"tl; dr; A combinator data component that installs Feast , a feature store. Introduction Test Drive Usage","title":"Feast"},{"location":"components/data/feast/#introduction","text":"Feast is an open-source feature store. A feature store allows you to manage, govern, and trace features derived from raw data. This is useful because it helps to unify and standardise, which reduces waste, improves quality, and makes models more reproducible. Feast does not perform any computation. You can think of it as a meta-database; a database that manages other databases. It effectively creates a cache of feature data, keyed by time. The Feast libraries and CLIs provide a consistent way of pushing or streaming new data into the cache. Downstream systems use a similar interface to access point-in-time data. Learn more about feast in the documentation.","title":"Introduction"},{"location":"components/data/feast/#test-drive","text":"The fastest way to get started is to use the test drive functionality provided by TestFaster . Click on the \"Launch Test Drive\" button below (opens a new window). Launch Test Drive","title":"Test Drive"},{"location":"components/data/feast/#launch-jupyter","text":"Once the component has launched, click on the Jupyter link. Feast does not come with a UI. You will use Jupyter to interact with Feast via its API.","title":"Launch Jupyter"},{"location":"components/data/feast/#example-notebook","text":"Once inside Jupyter, browse to the minimal notebook, which is the official example . Follow the instructions in the notebook.","title":"Example Notebook"},{"location":"components/data/feast/#usage","text":"","title":"Usage"},{"location":"components/data/feast/#prerequisites","text":"Start by preparing your Kubernetes cluster using one of the infrastructure components or use your own cluster.","title":"Prerequisites"},{"location":"components/data/feast/#component-usage","text":"module \"feast\" { source = \"combinator-ml/feast/k8s\" # Optional settings go here } See the full configuration options below.","title":"Component Usage"},{"location":"components/data/feast/#requirements","text":"No requirements.","title":"Requirements"},{"location":"components/data/feast/#providers","text":"Name Version helm n/a kubernetes n/a random n/a","title":"Providers"},{"location":"components/data/feast/#modules","text":"No Modules.","title":"Modules"},{"location":"components/data/feast/#resources","text":"Name helm_release kubernetes_namespace kubernetes_secret random_password","title":"Resources"},{"location":"components/data/feast/#inputs","text":"Name Description Type Default Required name_prefix Prefix to be used when naming the different components of Feast string \"combinator\" no namespace (Optional) The namespace to install into. Defaults to feast. string \"feast\" no","title":"Inputs"},{"location":"components/data/feast/#outputs","text":"No output.","title":"Outputs"},{"location":"components/data/istio/","text":"Istio The Istio component allows you to create a network mesh. This is a common dependency for other projects. Website","title":"Istio"},{"location":"components/data/istio/#istio","text":"The Istio component allows you to create a network mesh. This is a common dependency for other projects. Website","title":"Istio"},{"location":"components/data/minio/","text":"Minio The Minio component creates a cloud-native S3 compatible data store. Website","title":"Minio"},{"location":"components/data/minio/#minio","text":"The Minio component creates a cloud-native S3 compatible data store. Website","title":"Minio"},{"location":"components/data/pachyderm/","text":"Pachyderm tl; dr; A combinator data component that installs Pachyderm , a data lineage and pipelining solution. Introduction Test Drive Usage Introduction Pachyderm is an open-source-driven solution that provides data lineage and pipelines. Data lineage is important for _provenance_; knowing the origin of downstream assets. In ML, the assets are often models and the provenance describes how the model became to be. Precise knowledge of what a model was trained upon is important for disaster recovery, auditing, and robustness. Pipelines encode a process. This can be anything from automating pre-processing, to training and deploying models. Pachyderm's solution is unique beacuse it is backed by data lineage; i.e. data driven pipelines, not process driven ones. Test Drive The fastest way to get started is to use the test drive functionality provided by TestFaster . Click on the \"Launch Test Drive\" button below (opens a new window). Launch Test Drive Quick Start Pachyderm Tutorial Once the test drive has launched, click the two links to the left to get started with Pachyderm: Click the Jupyter link and launch the demo.ipynb notebook. Click on the Dashboard link to launch the Pachyderm Enterprise Dashboard. Usage Prerequisites Start by preparing your Kubernetes cluster using one of the infrastructure components or use your own cluster. Component Usage module \"pachyderm\" { source = \"combinator-ml/pachyderm/k8s\" # Optional settings go here } See the full configuration options below. Requirements Name Version helm ~> 2.1.2 kubernetes ~> 2.2.0 null ~> 3.1.0 Providers Name Version helm ~> 2.1.2 kubernetes ~> 2.2.0 Modules No Modules. Resources Name helm_release kubernetes_namespace Inputs Name Description Type Default Required namespace (Optional) The namespace to install the release into. string \"pachyderm\" no values (Optional) List of values in raw yaml to pass to helm. See https://github.com/pachyderm/helmchart/blob/master/pachyderm/values.yaml. list(string) [ \"tls:\\n certName: null # Disable TLS\\n create: null # Disable TLS\\npachd:\\n logLevel: debug\\n storage:\\n backend: LOCAL\\n\" ] no Outputs Name Description namespace Namespace is the kubernetes namespace of the release.","title":"Pachyderm"},{"location":"components/data/pachyderm/#pachyderm","text":"tl; dr; A combinator data component that installs Pachyderm , a data lineage and pipelining solution. Introduction Test Drive Usage","title":"Pachyderm"},{"location":"components/data/pachyderm/#introduction","text":"Pachyderm is an open-source-driven solution that provides data lineage and pipelines. Data lineage is important for _provenance_; knowing the origin of downstream assets. In ML, the assets are often models and the provenance describes how the model became to be. Precise knowledge of what a model was trained upon is important for disaster recovery, auditing, and robustness. Pipelines encode a process. This can be anything from automating pre-processing, to training and deploying models. Pachyderm's solution is unique beacuse it is backed by data lineage; i.e. data driven pipelines, not process driven ones.","title":"Introduction"},{"location":"components/data/pachyderm/#test-drive","text":"The fastest way to get started is to use the test drive functionality provided by TestFaster . Click on the \"Launch Test Drive\" button below (opens a new window). Launch Test Drive","title":"Test Drive"},{"location":"components/data/pachyderm/#quick-start-pachyderm-tutorial","text":"Once the test drive has launched, click the two links to the left to get started with Pachyderm: Click the Jupyter link and launch the demo.ipynb notebook. Click on the Dashboard link to launch the Pachyderm Enterprise Dashboard.","title":"Quick Start Pachyderm Tutorial"},{"location":"components/data/pachyderm/#usage","text":"","title":"Usage"},{"location":"components/data/pachyderm/#prerequisites","text":"Start by preparing your Kubernetes cluster using one of the infrastructure components or use your own cluster.","title":"Prerequisites"},{"location":"components/data/pachyderm/#component-usage","text":"module \"pachyderm\" { source = \"combinator-ml/pachyderm/k8s\" # Optional settings go here } See the full configuration options below.","title":"Component Usage"},{"location":"components/data/pachyderm/#requirements","text":"Name Version helm ~> 2.1.2 kubernetes ~> 2.2.0 null ~> 3.1.0","title":"Requirements"},{"location":"components/data/pachyderm/#providers","text":"Name Version helm ~> 2.1.2 kubernetes ~> 2.2.0","title":"Providers"},{"location":"components/data/pachyderm/#modules","text":"No Modules.","title":"Modules"},{"location":"components/data/pachyderm/#resources","text":"Name helm_release kubernetes_namespace","title":"Resources"},{"location":"components/data/pachyderm/#inputs","text":"Name Description Type Default Required namespace (Optional) The namespace to install the release into. string \"pachyderm\" no values (Optional) List of values in raw yaml to pass to helm. See https://github.com/pachyderm/helmchart/blob/master/pachyderm/values.yaml. list(string) [ \"tls:\\n certName: null # Disable TLS\\n create: null # Disable TLS\\npachd:\\n logLevel: debug\\n storage:\\n backend: LOCAL\\n\" ] no","title":"Inputs"},{"location":"components/data/pachyderm/#outputs","text":"Name Description namespace Namespace is the kubernetes namespace of the release.","title":"Outputs"},{"location":"components/deploy/seldon/","text":"Seldon tl; dr; A combinator data component that installs Seldon-Core , an ML serving framework. Introduction Test Drive Usage Introduction Seldon-Core is an open-source model serving and monitoring framework. It allows you to deploy your ML models so that they can be consumed by users over consistent REST APIs. In addition, other Seldon tools allow you to monitor your model in production. Learn more about Seldon-Core in the repo. Test Drive The fastest way to get started is to use the test drive functionality provided by TestFaster . Click on the \"Launch Test Drive\" button below (opens a new window). Launch Test Drive Launch Jupyter Once the component has launched, click on the Jupyter link. Seldon-Core does not have a UI by default. You will use Jupyter to interact with Seldon-Core via its Kubernetes API. Example Notebook Once inside Jupyter, browse to the demo notebook, which comes from the official quick start guide . Follow the instructions in the notebook to deploy a pre-trained model. Usage Prerequisites Start by preparing your Kubernetes cluster using one of the infrastructure components or use your own cluster. Component Usage module \"seldon\" { source = \"combinator-ml/seldon/k8s\" # Optional settings go here } See the full configuration options below. Requirements Name Version helm >= 2.0.0 kubectl >= 1.7.0 kubernetes >= 2.0.0 Providers Name Version helm >= 2.0.0 kubectl >= 1.7.0 kubernetes >= 2.0.0 Modules Name Source Version istio combinator-ml/istio/k8s 0.0.1 Resources Name helm_release kubectl_file_documents kubectl_manifest kubernetes_namespace Inputs Name Description Type Default Required enable_example_seldon_deployment Enable an example seldon deployment bool true no enable_seldon_gateway Create an istio gateway for seldon bool true no seldon_core_operator_namespace (Optional) The namespace to install the minio operator into. Defaults to minio-operator string \"seldon-system\" no seldon_core_values (Optional) List of values in raw yaml to pass to helm. list(string) [] no Outputs No output.","title":"Seldon"},{"location":"components/deploy/seldon/#seldon","text":"tl; dr; A combinator data component that installs Seldon-Core , an ML serving framework. Introduction Test Drive Usage","title":"Seldon"},{"location":"components/deploy/seldon/#introduction","text":"Seldon-Core is an open-source model serving and monitoring framework. It allows you to deploy your ML models so that they can be consumed by users over consistent REST APIs. In addition, other Seldon tools allow you to monitor your model in production. Learn more about Seldon-Core in the repo.","title":"Introduction"},{"location":"components/deploy/seldon/#test-drive","text":"The fastest way to get started is to use the test drive functionality provided by TestFaster . Click on the \"Launch Test Drive\" button below (opens a new window). Launch Test Drive","title":"Test Drive"},{"location":"components/deploy/seldon/#launch-jupyter","text":"Once the component has launched, click on the Jupyter link. Seldon-Core does not have a UI by default. You will use Jupyter to interact with Seldon-Core via its Kubernetes API.","title":"Launch Jupyter"},{"location":"components/deploy/seldon/#example-notebook","text":"Once inside Jupyter, browse to the demo notebook, which comes from the official quick start guide . Follow the instructions in the notebook to deploy a pre-trained model.","title":"Example Notebook"},{"location":"components/deploy/seldon/#usage","text":"","title":"Usage"},{"location":"components/deploy/seldon/#prerequisites","text":"Start by preparing your Kubernetes cluster using one of the infrastructure components or use your own cluster.","title":"Prerequisites"},{"location":"components/deploy/seldon/#component-usage","text":"module \"seldon\" { source = \"combinator-ml/seldon/k8s\" # Optional settings go here } See the full configuration options below.","title":"Component Usage"},{"location":"components/deploy/seldon/#requirements","text":"Name Version helm >= 2.0.0 kubectl >= 1.7.0 kubernetes >= 2.0.0","title":"Requirements"},{"location":"components/deploy/seldon/#providers","text":"Name Version helm >= 2.0.0 kubectl >= 1.7.0 kubernetes >= 2.0.0","title":"Providers"},{"location":"components/deploy/seldon/#modules","text":"Name Source Version istio combinator-ml/istio/k8s 0.0.1","title":"Modules"},{"location":"components/deploy/seldon/#resources","text":"Name helm_release kubectl_file_documents kubectl_manifest kubernetes_namespace","title":"Resources"},{"location":"components/deploy/seldon/#inputs","text":"Name Description Type Default Required enable_example_seldon_deployment Enable an example seldon deployment bool true no enable_seldon_gateway Create an istio gateway for seldon bool true no seldon_core_operator_namespace (Optional) The namespace to install the minio operator into. Defaults to minio-operator string \"seldon-system\" no seldon_core_values (Optional) List of values in raw yaml to pass to helm. list(string) [] no","title":"Inputs"},{"location":"components/deploy/seldon/#outputs","text":"No output.","title":"Outputs"},{"location":"components/develop/jupyter/","text":"terraform-k8s-jupyter tl; dr; A combinator data component that provides a single Jupyter instance, a notebook provider. Introduction Test Drive Usage Introduction Jupyter is an open-source notebook host. Data scientists use notebooks to research, develop, and document their solutions. However, it is also very useful to use when demonstrating other products where the user persona is an engineer or scientist. Hence, this component is generally used within other stacks to enable demos. The functionality of this component is intended to help provide those demos. If you are more interested in a notebook platform, then check out Jupyter Hub , or any cloud vendor notebook hosting solution. Test Drive The fastest way to get started is to use the test drive functionality provided by TestFaster . Click on the \"Launch Test Drive\" button below (opens a new window). Launch Test Drive Launch Jupyter Once the component has launched, click on the Jupyter link. Once inside Jupyter, explore and try some demos. Prerequisites Start by preparing your Kubernetes cluster using one of the infrastructure components or use your own cluster. Component Usage module \"feast\" { source = \"combinator-ml/jupyter/k8s\" # Optional settings go here } See the full configuration options below. Requirements No requirements. Providers Name Version kubernetes n/a Modules No Modules. Resources Name kubernetes_deployment kubernetes_namespace Inputs Name Description Type Default Required image Docker image to use string \"jupyter/scipy-notebook:python-3.9.2\" no name_prefix Prefix to be used when naming the different components. string \"combinator\" no namespace The namespace to install into. string \"jupyter\" no Outputs No output.","title":"terraform-k8s-jupyter"},{"location":"components/develop/jupyter/#terraform-k8s-jupyter","text":"tl; dr; A combinator data component that provides a single Jupyter instance, a notebook provider. Introduction Test Drive Usage","title":"terraform-k8s-jupyter"},{"location":"components/develop/jupyter/#introduction","text":"Jupyter is an open-source notebook host. Data scientists use notebooks to research, develop, and document their solutions. However, it is also very useful to use when demonstrating other products where the user persona is an engineer or scientist. Hence, this component is generally used within other stacks to enable demos. The functionality of this component is intended to help provide those demos. If you are more interested in a notebook platform, then check out Jupyter Hub , or any cloud vendor notebook hosting solution.","title":"Introduction"},{"location":"components/develop/jupyter/#test-drive","text":"The fastest way to get started is to use the test drive functionality provided by TestFaster . Click on the \"Launch Test Drive\" button below (opens a new window). Launch Test Drive","title":"Test Drive"},{"location":"components/develop/jupyter/#launch-jupyter","text":"Once the component has launched, click on the Jupyter link. Once inside Jupyter, explore and try some demos.","title":"Launch Jupyter"},{"location":"components/develop/jupyter/#prerequisites","text":"Start by preparing your Kubernetes cluster using one of the infrastructure components or use your own cluster.","title":"Prerequisites"},{"location":"components/develop/jupyter/#component-usage","text":"module \"feast\" { source = \"combinator-ml/jupyter/k8s\" # Optional settings go here } See the full configuration options below.","title":"Component Usage"},{"location":"components/develop/jupyter/#requirements","text":"No requirements.","title":"Requirements"},{"location":"components/develop/jupyter/#providers","text":"Name Version kubernetes n/a","title":"Providers"},{"location":"components/develop/jupyter/#modules","text":"No Modules.","title":"Modules"},{"location":"components/develop/jupyter/#resources","text":"Name kubernetes_deployment kubernetes_namespace","title":"Resources"},{"location":"components/develop/jupyter/#inputs","text":"Name Description Type Default Required image Docker image to use string \"jupyter/scipy-notebook:python-3.9.2\" no name_prefix Prefix to be used when naming the different components. string \"combinator\" no namespace The namespace to install into. string \"jupyter\" no","title":"Inputs"},{"location":"components/develop/jupyter/#outputs","text":"No output.","title":"Outputs"},{"location":"components/develop/kubeflow/","text":"Kubeflow tl; dr; A combinator governance component that provides Kubeflow , a pipelining tool, Jupyter host, and hyperparameter tuner. Introduction Test Drive Usage Introduction Kubeflow is an open-source MLOps platform that combines Jupyter hosting, ML pipelining, and hyperparameter tuning. It is packaged into a single UI to help data scientists train their ML models. Kubeflow Pipelines (KFP) in particular, has emerged as one eminent ML pipelinging technology, mainly thanks to the managed hosting in various clouds. Its opinionated ML-specific API helps data scientists and ML engineers develop robust, repeatable pipelines. Kubeflow Version This installation uses Kubeflow version 1.2, which is now out of date. Status and Recommendations For Testing Only This installation method is not recommended for use. It required a lot of work-arounds that are not suitable for production use. Please refer to the official documentation for production installation instructions. Test Drive The fastest way to get started is to use the test drive functionality provided by TestFaster . Click on the \"Launch Test Drive\" button below (opens a new window). Launch Test Drive Usage Prerequisites Start by preparing your Kubernetes cluster using one of the infrastructure components or use your own cluster. Component Usage module \"kubeflow\" { source = \"combinator-ml/kubeflow/k8s\" # Optional settings go here } See the full configuration options below. Instructions Kubeflow is big, so it can take some time to start. Once it does connect to the istio ingress gateway service. Once you see the login screen, the username is admin@kubeflow.org and the password is 12341234 . Requirements Name Version terraform >= 0.13 helm = 2.2.0 k8s = 0.9.1 kubernetes = 2.3.2 Providers No provider. Modules Name Source Version kubeflow ./terraform-module-kubeflow Resources No resources. Inputs No input. Outputs No output.","title":"Kubeflow"},{"location":"components/develop/kubeflow/#kubeflow","text":"tl; dr; A combinator governance component that provides Kubeflow , a pipelining tool, Jupyter host, and hyperparameter tuner. Introduction Test Drive Usage","title":"Kubeflow"},{"location":"components/develop/kubeflow/#introduction","text":"Kubeflow is an open-source MLOps platform that combines Jupyter hosting, ML pipelining, and hyperparameter tuning. It is packaged into a single UI to help data scientists train their ML models. Kubeflow Pipelines (KFP) in particular, has emerged as one eminent ML pipelinging technology, mainly thanks to the managed hosting in various clouds. Its opinionated ML-specific API helps data scientists and ML engineers develop robust, repeatable pipelines.","title":"Introduction"},{"location":"components/develop/kubeflow/#kubeflow-version","text":"This installation uses Kubeflow version 1.2, which is now out of date.","title":"Kubeflow Version"},{"location":"components/develop/kubeflow/#status-and-recommendations","text":"For Testing Only This installation method is not recommended for use. It required a lot of work-arounds that are not suitable for production use. Please refer to the official documentation for production installation instructions.","title":"Status and Recommendations"},{"location":"components/develop/kubeflow/#test-drive","text":"The fastest way to get started is to use the test drive functionality provided by TestFaster . Click on the \"Launch Test Drive\" button below (opens a new window). Launch Test Drive","title":"Test Drive"},{"location":"components/develop/kubeflow/#usage","text":"","title":"Usage"},{"location":"components/develop/kubeflow/#prerequisites","text":"Start by preparing your Kubernetes cluster using one of the infrastructure components or use your own cluster.","title":"Prerequisites"},{"location":"components/develop/kubeflow/#component-usage","text":"module \"kubeflow\" { source = \"combinator-ml/kubeflow/k8s\" # Optional settings go here } See the full configuration options below.","title":"Component Usage"},{"location":"components/develop/kubeflow/#instructions","text":"Kubeflow is big, so it can take some time to start. Once it does connect to the istio ingress gateway service. Once you see the login screen, the username is admin@kubeflow.org and the password is 12341234 .","title":"Instructions"},{"location":"components/develop/kubeflow/#requirements","text":"Name Version terraform >= 0.13 helm = 2.2.0 k8s = 0.9.1 kubernetes = 2.3.2","title":"Requirements"},{"location":"components/develop/kubeflow/#providers","text":"No provider.","title":"Providers"},{"location":"components/develop/kubeflow/#modules","text":"Name Source Version kubeflow ./terraform-module-kubeflow","title":"Modules"},{"location":"components/develop/kubeflow/#resources","text":"No resources.","title":"Resources"},{"location":"components/develop/kubeflow/#inputs","text":"No input.","title":"Inputs"},{"location":"components/develop/kubeflow/#outputs","text":"No output.","title":"Outputs"},{"location":"components/govern/mlflow/","text":"MLFlow tl; dr; A combinator governance component that provides a hosted MLFlow server, a model repository and experiment tracker. Introduction Test Drive Usage Introduction MLFlow is an open-source model registry and experiment tracker. Data scientists leverage this hosted service to store the results of their experiments and persist final artifacts like model parameters and weights. This provides a centralised catalogue of models and experiments, which is useful for organizational purposes and sharing work. MLFlow also comes with limited serving capabilities, although that is not it's core aim. Test Drive The fastest way to get started is to use the test drive functionality provided by TestFaster . Click on the \"Launch Test Drive\" button below (opens a new window). Launch Test Drive Usage Prerequisites Start by preparing your Kubernetes cluster using one of the infrastructure components or use your own cluster. Component Usage module \"mlflow\" { source = \"combinator-ml/mlflow/k8s\" # Optional settings go here } See the full configuration options below. Requirements Name Version terraform >= 0.14 helm >= 2.0.0 kubernetes >= 2.0.0 Providers Name Version helm >= 2.0.0 kubernetes >= 2.0.0 Modules No Modules. Resources Name helm_release kubernetes_secret Inputs Name Description Type Default Required name_prefix Prefix to be used when naming the different components. string \"combinator\" no namespace The namespace to install into. string \"mlflow\" no Outputs No output.","title":"MLFlow"},{"location":"components/govern/mlflow/#mlflow","text":"tl; dr; A combinator governance component that provides a hosted MLFlow server, a model repository and experiment tracker. Introduction Test Drive Usage","title":"MLFlow"},{"location":"components/govern/mlflow/#introduction","text":"MLFlow is an open-source model registry and experiment tracker. Data scientists leverage this hosted service to store the results of their experiments and persist final artifacts like model parameters and weights. This provides a centralised catalogue of models and experiments, which is useful for organizational purposes and sharing work. MLFlow also comes with limited serving capabilities, although that is not it's core aim.","title":"Introduction"},{"location":"components/govern/mlflow/#test-drive","text":"The fastest way to get started is to use the test drive functionality provided by TestFaster . Click on the \"Launch Test Drive\" button below (opens a new window). Launch Test Drive","title":"Test Drive"},{"location":"components/govern/mlflow/#usage","text":"","title":"Usage"},{"location":"components/govern/mlflow/#prerequisites","text":"Start by preparing your Kubernetes cluster using one of the infrastructure components or use your own cluster.","title":"Prerequisites"},{"location":"components/govern/mlflow/#component-usage","text":"module \"mlflow\" { source = \"combinator-ml/mlflow/k8s\" # Optional settings go here } See the full configuration options below.","title":"Component Usage"},{"location":"components/govern/mlflow/#requirements","text":"Name Version terraform >= 0.14 helm >= 2.0.0 kubernetes >= 2.0.0","title":"Requirements"},{"location":"components/govern/mlflow/#providers","text":"Name Version helm >= 2.0.0 kubernetes >= 2.0.0","title":"Providers"},{"location":"components/govern/mlflow/#modules","text":"No Modules.","title":"Modules"},{"location":"components/govern/mlflow/#resources","text":"Name helm_release kubernetes_secret","title":"Resources"},{"location":"components/govern/mlflow/#inputs","text":"Name Description Type Default Required name_prefix Prefix to be used when naming the different components. string \"combinator\" no namespace The namespace to install into. string \"mlflow\" no","title":"Inputs"},{"location":"components/govern/mlflow/#outputs","text":"No output.","title":"Outputs"},{"location":"infrastructure/google/","text":"Google Google offer Kubernetes clusters in their Google Kubernetes Engine . This component creates a cluster in GKE. Component Website Prerequisites To use this component you will need a GCP account, be authorised to create infrastructure and a GCP project id. Costs This runs in a single zone, which means it falls under GCP's free management tier and you don't have to pay for management nodes, only worker nodes. After the first zone you pay, and that's when it gets expensive. See the GCP pricing pages for more information. Usage module \"terraform-google-kubernetes\" { source = \"combinator-ml/kubernetes/google\" gcp_project_id = \"your-gpc-project-id\" }","title":"Google"},{"location":"infrastructure/google/#google","text":"Google offer Kubernetes clusters in their Google Kubernetes Engine . This component creates a cluster in GKE. Component Website","title":"Google"},{"location":"infrastructure/google/#prerequisites","text":"To use this component you will need a GCP account, be authorised to create infrastructure and a GCP project id.","title":"Prerequisites"},{"location":"infrastructure/google/#costs","text":"This runs in a single zone, which means it falls under GCP's free management tier and you don't have to pay for management nodes, only worker nodes. After the first zone you pay, and that's when it gets expensive. See the GCP pricing pages for more information.","title":"Costs"},{"location":"infrastructure/google/#usage","text":"module \"terraform-google-kubernetes\" { source = \"combinator-ml/kubernetes/google\" gcp_project_id = \"your-gpc-project-id\" }","title":"Usage"},{"location":"infrastructure/introduction/","text":"Introduction Most of the combinator components and stacks target Kubernetes. This means you need a running Kubernetes cluster. The infrastructure-specific components provide a simple way to create Kubernetes clusters in all of the major clouds. This pages will also describe how to setup Kubernetes locally. Cloud Kubernetes Components Google GKE","title":"Introduction"},{"location":"infrastructure/introduction/#introduction","text":"Most of the combinator components and stacks target Kubernetes. This means you need a running Kubernetes cluster. The infrastructure-specific components provide a simple way to create Kubernetes clusters in all of the major clouds. This pages will also describe how to setup Kubernetes locally.","title":"Introduction"},{"location":"infrastructure/introduction/#cloud-kubernetes-components","text":"Google GKE","title":"Cloud Kubernetes Components"},{"location":"stacks/bodywork-mlflow/","text":"Bodywork + MLflow This tutorial enables you to experiment with Bodywork and MLflow combined into a single open-source MLOps stack. Bodywork is a tool that focuses on the deployment of machine learning projects to Kubernetes. MLflow is a tool for managing the machine learning lifecycle (tracking metrics and managing ML arteacts, such as trained models). We have developed an example train-and-serve pipeline to demonstrate Bodywork and MLflow working side-by-side, which you can explore in this GitHub repository . The pipeline uses MLflow to track the training metrics and manage trained models. The pipeline consists of two stages, defined in two executable Python modules: train_model.py - run a batch job to train a model, logging metrics and registering models to MLflow. serve_model.py - loads the latest 'production' model from MLflow and then starts a simple Flask app to handle requests for scoring data. The details of the deployment are described in the bodywork.yaml configuration file. When a deployment is triggered, Bodywork instructs Kubernetes to start pre-built Bodywork containers , that pull the code from the demo project's Git repo and run the executable Python modules. Each stage is associated with one Python module and is run, in isolation, in it's own container. Launch the test drive below and follow the steps to see this pipeline in action! Step 0 - Launch the Test Drive Note: the test drive doesn't work in Safari yet. Please use Chrome or Firefox for now! Also please note it won't work in Private/Incognito windows. Use the following test drive to launch a temporary Kubernetes cluster with the tutorial running in it: function toggle(el) { var x = document.getElementById(el); if (x.style.display === \"none\") { x.style.display = \"block\"; } else { x.style.display = \"none\"; } } Launch Test Drive At busy times, you may need to wait a few minutes for a test drive environment to become available. Note that the environment will shut down automatically 1 hour after you start using it. Step 1 - Deploy the Pipeline To test the deployment using a local workflow-controller that streams logs to stdout, run, $ bodywork workflow \\ --namespace=bodywork \\ https://github.com/bodywork-ml/bodywork-pipeline-with-mlflow \\ master Once the deployment has completed, browse to the MLflow UI to check that the model metrics that were logged to the iris-classification experiment during training, and to confirm that the trained model, iris-classifier--sklearn-decision-tree , was registered and promoted to 'production'. Step 2 - Test the Scoring Service Requests to score data can now be sent to the scoring service. Try the following in the shell, $ curl http://localhost:31380/bodywork/bodywork-mlflow-demo--scoring-service/iris/v1/score \\ --request POST \\ --header \"Content-Type: application/json\" \\ --data '{\"sepal_length\": 5.1, \"sepal_width\": 3.5, \"petal_length\": 1.4, \"petal_width\": 0.2}' Which should return, { \"species_prediction\" : \"setosa\" , \"probabilities\" : \"setosa=1.0|versicolor=0.0|virginica=0.0\" , \"model_info\" : \"DecisionTreeClassifier(class_weight='balanced', random_state=42)\" } According to how the payload has been defined in the serve_model.py module. Running the ML Pipeline on a Schedule If you're happy with the test results, you can schedule the workflow-controller to operate remotely on the cluster, on a pre-defined schedule. For example, to setup the the workflow to run every hour, use the following command, $ bodywork cronjob create \\ --namespace=bodywork \\ --name=train-and-deploy \\ --schedule=\"0 * * * *\" \\ --git-repo-url=https://github.com/bodywork-ml/bodywork-bodywork-mlflow-demo-project \\ --git-repo-branch=master Each scheduled workflow will attempt to re-run the batch-job, as defined by the state of this repository's master branch at the time of execution. To get the execution history for all train-and-deploy jobs use, $ bodywork cronjob history \\ --namespace=bodywork \\ --name=train-and-deploy Which should return output along the lines of, JOB_NAME START_TIME COMPLETION_TIME ACTIVE SUCCEEDED FAILED train-and-deploy-1605214260 2020-11-12 20:51:04+00:00 2020-11-12 20:52:34+00:00 0 1 0 Cleaning Up To clean-up the deployment in its entirety, delete the namespace using Kubectl - e.g. by running, $ kubectl delete ns bodywork Make this Project Your Own This repository is a GitHub template repository that can be automatically copied into your own GitHub account by clicking the Use this template button above. After you've cloned the template project, use official Bodywork documentation to help modify the project to meet your own requirements.","title":"Bodywork + MLflow"},{"location":"stacks/bodywork-mlflow/#bodywork-mlflow","text":"This tutorial enables you to experiment with Bodywork and MLflow combined into a single open-source MLOps stack. Bodywork is a tool that focuses on the deployment of machine learning projects to Kubernetes. MLflow is a tool for managing the machine learning lifecycle (tracking metrics and managing ML arteacts, such as trained models). We have developed an example train-and-serve pipeline to demonstrate Bodywork and MLflow working side-by-side, which you can explore in this GitHub repository . The pipeline uses MLflow to track the training metrics and manage trained models. The pipeline consists of two stages, defined in two executable Python modules: train_model.py - run a batch job to train a model, logging metrics and registering models to MLflow. serve_model.py - loads the latest 'production' model from MLflow and then starts a simple Flask app to handle requests for scoring data. The details of the deployment are described in the bodywork.yaml configuration file. When a deployment is triggered, Bodywork instructs Kubernetes to start pre-built Bodywork containers , that pull the code from the demo project's Git repo and run the executable Python modules. Each stage is associated with one Python module and is run, in isolation, in it's own container. Launch the test drive below and follow the steps to see this pipeline in action!","title":"Bodywork + MLflow"},{"location":"stacks/bodywork-mlflow/#step-0-launch-the-test-drive","text":"Note: the test drive doesn't work in Safari yet. Please use Chrome or Firefox for now! Also please note it won't work in Private/Incognito windows. Use the following test drive to launch a temporary Kubernetes cluster with the tutorial running in it: function toggle(el) { var x = document.getElementById(el); if (x.style.display === \"none\") { x.style.display = \"block\"; } else { x.style.display = \"none\"; } } Launch Test Drive At busy times, you may need to wait a few minutes for a test drive environment to become available. Note that the environment will shut down automatically 1 hour after you start using it.","title":"Step 0 - Launch the Test Drive"},{"location":"stacks/bodywork-mlflow/#step-1-deploy-the-pipeline","text":"To test the deployment using a local workflow-controller that streams logs to stdout, run, $ bodywork workflow \\ --namespace=bodywork \\ https://github.com/bodywork-ml/bodywork-pipeline-with-mlflow \\ master Once the deployment has completed, browse to the MLflow UI to check that the model metrics that were logged to the iris-classification experiment during training, and to confirm that the trained model, iris-classifier--sklearn-decision-tree , was registered and promoted to 'production'.","title":"Step 1 - Deploy the Pipeline"},{"location":"stacks/bodywork-mlflow/#step-2-test-the-scoring-service","text":"Requests to score data can now be sent to the scoring service. Try the following in the shell, $ curl http://localhost:31380/bodywork/bodywork-mlflow-demo--scoring-service/iris/v1/score \\ --request POST \\ --header \"Content-Type: application/json\" \\ --data '{\"sepal_length\": 5.1, \"sepal_width\": 3.5, \"petal_length\": 1.4, \"petal_width\": 0.2}' Which should return, { \"species_prediction\" : \"setosa\" , \"probabilities\" : \"setosa=1.0|versicolor=0.0|virginica=0.0\" , \"model_info\" : \"DecisionTreeClassifier(class_weight='balanced', random_state=42)\" } According to how the payload has been defined in the serve_model.py module.","title":"Step 2 - Test the Scoring Service"},{"location":"stacks/bodywork-mlflow/#running-the-ml-pipeline-on-a-schedule","text":"If you're happy with the test results, you can schedule the workflow-controller to operate remotely on the cluster, on a pre-defined schedule. For example, to setup the the workflow to run every hour, use the following command, $ bodywork cronjob create \\ --namespace=bodywork \\ --name=train-and-deploy \\ --schedule=\"0 * * * *\" \\ --git-repo-url=https://github.com/bodywork-ml/bodywork-bodywork-mlflow-demo-project \\ --git-repo-branch=master Each scheduled workflow will attempt to re-run the batch-job, as defined by the state of this repository's master branch at the time of execution. To get the execution history for all train-and-deploy jobs use, $ bodywork cronjob history \\ --namespace=bodywork \\ --name=train-and-deploy Which should return output along the lines of, JOB_NAME START_TIME COMPLETION_TIME ACTIVE SUCCEEDED FAILED train-and-deploy-1605214260 2020-11-12 20:51:04+00:00 2020-11-12 20:52:34+00:00 0 1 0","title":"Running the ML Pipeline on a Schedule"},{"location":"stacks/bodywork-mlflow/#cleaning-up","text":"To clean-up the deployment in its entirety, delete the namespace using Kubectl - e.g. by running, $ kubectl delete ns bodywork","title":"Cleaning Up"},{"location":"stacks/bodywork-mlflow/#make-this-project-your-own","text":"This repository is a GitHub template repository that can be automatically copied into your own GitHub account by clicking the Use this template button above. After you've cloned the template project, use official Bodywork documentation to help modify the project to meet your own requirements.","title":"Make this Project Your Own"},{"location":"stacks/introduction/","text":"Introduction Combinator stacks are built from two or more components . A stack represents a common combination of tools to provide end-to-end ML functionality. A specific stack is formed from an opinionated selection of components , so there may be cross-over between stacks.","title":"Introduction"},{"location":"stacks/introduction/#introduction","text":"Combinator stacks are built from two or more components . A stack represents a common combination of tools to provide end-to-end ML functionality. A specific stack is formed from an opinionated selection of components , so there may be cross-over between stacks.","title":"Introduction"},{"location":"stacks/kubeflow-mlflow/","text":"Kubeflow + MLFlow tl; dr; A combinator stack that provides Kubeflow and MLFlow . Introduction Test Drive Usage Introduction Kubeflow is an open-source MLOps platform that combines Jupyter hosting, ML pipelining, and hyperparameter tuning. It is packaged into a single UI to help data scientists train their ML models. Kubeflow Pipelines (KFP) in particular, has emerged as one eminent ML pipelinging technology, mainly thanks to the managed hosting in various clouds. Its opinionated ML-specific API helps data scientists and ML engineers develop robust, repeatable pipelines. This stack adds MLflow for model management and makes it easy to log models to MLflow from kubeflow notebooks and pipelines. Kubeflow Version This installation uses Kubeflow version 1.2, which is now out of date. Status and Recommendations For Testing Only This installation method is not recommended for use. It required a lot of work-arounds that are not suitable for production use. Please refer to the official documentation for production installation instructions. Test Drive The fastest way to get started is to use the test drive functionality provided by TestFaster . Click on the \"Launch Test Drive\" button below (opens a new window). Launch Test Drive Usage Prerequisites Start by preparing your Kubernetes cluster using one of the infrastructure components or use your own cluster. Component Usage module \"kubeflow_mlflow_stack\" { source = \"combinator-ml/stack-kubeflow-mlflow/k8s\" # Optional settings go here } See the full configuration options below. Instructions Kubeflow is big, so it can take some time to start. Once it does connect to the istio ingress gateway service. Once you see the login screen, the username is admin@kubeflow.org and the password is 12341234 . Requirements Name Version terraform >= 0.13 helm >= 2.2.0 k8s >= 0.9.1 kubernetes >= 2.3.2 Providers Name Version k8s >= 0.9.1 kubernetes >= 2.3.2 null n/a time n/a Modules Name Source Version kubeflow combinator-ml/kubeflow/k8s 0.0.2 mlflow combinator-ml/mlflow/k8s 0.0.3 Resources Name k8s_manifest kubernetes_cluster_role_binding null_resource time_sleep Inputs Name Description Type Default Required mlflow_namespace (Optional) The namespace to install into. string \"mlflow\" no Outputs No output.","title":"Kubeflow + MLFlow"},{"location":"stacks/kubeflow-mlflow/#kubeflow-mlflow","text":"tl; dr; A combinator stack that provides Kubeflow and MLFlow . Introduction Test Drive Usage","title":"Kubeflow + MLFlow"},{"location":"stacks/kubeflow-mlflow/#introduction","text":"Kubeflow is an open-source MLOps platform that combines Jupyter hosting, ML pipelining, and hyperparameter tuning. It is packaged into a single UI to help data scientists train their ML models. Kubeflow Pipelines (KFP) in particular, has emerged as one eminent ML pipelinging technology, mainly thanks to the managed hosting in various clouds. Its opinionated ML-specific API helps data scientists and ML engineers develop robust, repeatable pipelines. This stack adds MLflow for model management and makes it easy to log models to MLflow from kubeflow notebooks and pipelines.","title":"Introduction"},{"location":"stacks/kubeflow-mlflow/#kubeflow-version","text":"This installation uses Kubeflow version 1.2, which is now out of date.","title":"Kubeflow Version"},{"location":"stacks/kubeflow-mlflow/#status-and-recommendations","text":"For Testing Only This installation method is not recommended for use. It required a lot of work-arounds that are not suitable for production use. Please refer to the official documentation for production installation instructions.","title":"Status and Recommendations"},{"location":"stacks/kubeflow-mlflow/#test-drive","text":"The fastest way to get started is to use the test drive functionality provided by TestFaster . Click on the \"Launch Test Drive\" button below (opens a new window). Launch Test Drive","title":"Test Drive"},{"location":"stacks/kubeflow-mlflow/#usage","text":"","title":"Usage"},{"location":"stacks/kubeflow-mlflow/#prerequisites","text":"Start by preparing your Kubernetes cluster using one of the infrastructure components or use your own cluster.","title":"Prerequisites"},{"location":"stacks/kubeflow-mlflow/#component-usage","text":"module \"kubeflow_mlflow_stack\" { source = \"combinator-ml/stack-kubeflow-mlflow/k8s\" # Optional settings go here } See the full configuration options below.","title":"Component Usage"},{"location":"stacks/kubeflow-mlflow/#instructions","text":"Kubeflow is big, so it can take some time to start. Once it does connect to the istio ingress gateway service. Once you see the login screen, the username is admin@kubeflow.org and the password is 12341234 .","title":"Instructions"},{"location":"stacks/kubeflow-mlflow/#requirements","text":"Name Version terraform >= 0.13 helm >= 2.2.0 k8s >= 0.9.1 kubernetes >= 2.3.2","title":"Requirements"},{"location":"stacks/kubeflow-mlflow/#providers","text":"Name Version k8s >= 0.9.1 kubernetes >= 2.3.2 null n/a time n/a","title":"Providers"},{"location":"stacks/kubeflow-mlflow/#modules","text":"Name Source Version kubeflow combinator-ml/kubeflow/k8s 0.0.2 mlflow combinator-ml/mlflow/k8s 0.0.3","title":"Modules"},{"location":"stacks/kubeflow-mlflow/#resources","text":"Name k8s_manifest kubernetes_cluster_role_binding null_resource time_sleep","title":"Resources"},{"location":"stacks/kubeflow-mlflow/#inputs","text":"Name Description Type Default Required mlflow_namespace (Optional) The namespace to install into. string \"mlflow\" no","title":"Inputs"},{"location":"stacks/kubeflow-mlflow/#outputs","text":"No output.","title":"Outputs"},{"location":"stacks/minio-pachyderm/","text":"Minio + Pachyderm Create a minio-backed Pachyderm cluster for data lineage and pipelines.","title":"Minio + Pachyderm"},{"location":"stacks/minio-pachyderm/#minio-pachyderm","text":"Create a minio-backed Pachyderm cluster for data lineage and pipelines.","title":"Minio + Pachyderm"}]}